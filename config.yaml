crawler:
  seed_urls:
    - https://example.com
    - https://example.org
    - https://httpbin.org
  max_depth: 5
  politeness_delay: 1.0
  max_concurrent_requests: 10
  request_timeout: 30
  retry_attempts: 3
  user_agent: "WebCrawler/1.0 (+https://github.com/alexnthnz/web-crawler)"
  respect_robots_txt: true
  allowed_domains: []  # Empty list means all domains allowed
  blocked_domains: []

database:
  type: "cassandra"  # or "file" for simple file storage
  cassandra:
    hosts: ["localhost"]
    port: 9042
    keyspace: "crawler_data"
    replication_factor: 1
  file:
    data_directory: "./data"

redis:
  host: "localhost"
  port: 6379
  db: 0
  password: null
  url_frontier_key: "crawler:url_frontier"
  processed_urls_key: "crawler:processed_urls"

logging:
  level: "INFO"
  file: "./logs/crawler.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

monitoring:
  prometheus_port: 8000
  metrics_enabled: true